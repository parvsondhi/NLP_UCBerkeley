{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using WordNet Hypernyms to Group Nouns Semantically #\n",
    "Code by Anna Swigart, ANLP 2015\n",
    "\n",
    "*** This code is intended to find main topics in the document by organizing nouns according to semantic class using WordNet.  It finds common hypernyms for many of the nouns, and squishes the hierarchy down for those shared hypernyms into one level. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.collocations import *\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Text Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cookbook corpus\n",
    "with open('cookbooks.txt', 'r') as text_file:\n",
    "    cookbooks_corpus = text_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use default tokenizer to start with\n",
    "def tokenize_text(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences\n",
    "    \n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]\n",
    "\n",
    "cookbook_sents = tokenize_text(cookbooks_corpus)\n",
    "brown_news_sents = brown.sents(categories='news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: Frequent Unigrams with Hypernyms and Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm takes in tokenized sentences, tags them the standard NLTK tagger, and then normalizes the words. (Note: in the original code, Anna used her special ngram tagger trained on her recipe collection.) Only nouns are included in the terms and cardinal numbers are excluded. The words are normalized by stemming (using the WordNet Lemmatizer) and cast to lowercase. Then, the 25 most common hyperterms of a list of the most frequent unigrams are extracted, along with a set of corresponding examples from the normalized corups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def freq_normed_unigrams(sents):\n",
    "    wnl = WordNetLemmatizer() # to get word stems\n",
    "    \n",
    "    tagged_POS_sents = [nltk.pos_tag(sent) for sent in sents] # tags sents\n",
    "    \n",
    "    normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           for word in sent \n",
    "                           if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           and word[0] not in punctuation # remove punctuation\n",
    "                           and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           and word[1].startswith('N')]  # include only nouns\n",
    "\n",
    "    top_normed_unigrams = [word for (word, count) in nltk.FreqDist(normed_tagged_words).most_common(40)]\n",
    "    return top_normed_unigrams\n",
    "\n",
    "def categories_from_hypernyms(sents):\n",
    "    termlist = freq_normed_unigrams(sents) # get top unigrams\n",
    "    hypterms = []\n",
    "    hypterms_dict = defaultdict(list)\n",
    "    for term in termlist:                  # for each term\n",
    "        s = wn.synsets(term.lower(), 'n')  # get its nominal synsets\n",
    "        for syn in s:                      # for each lemma synset\n",
    "            for hyp in syn.hypernyms():    # It has a list of hypernyms\n",
    "                hypterms = hypterms + [hyp.name]      # Extract the hypernym name and add to list\n",
    "                hypterms_dict[hyp.name].append(term)  # Extract examples and add them to dict\n",
    "    hypfd = nltk.FreqDist(hypterms)             # After going through all the nouns, print out the hypernyms \n",
    "    for (name, count) in hypfd.most_common(25):  # that have accumulated the most counts (have seen the most descendents)\n",
    "        print( name(), '({0})'.format(count))\n",
    "        print ('\\t', ', '.join(set(hypterms_dict[name])))  # show the children found for each hypernym\n",
    "        print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_period.n.01 (5)\n",
      "\t day, hour, time\n",
      "\n",
      "flavorer.n.01 (3)\n",
      "\t salt, pepper, herb\n",
      "\n",
      "helping.n.01 (3)\n",
      "\t slice, round, piece\n",
      "\n",
      "time_unit.n.01 (3)\n",
      "\t day, hour\n",
      "\n",
      "united_states_dry_unit.n.01 (2)\n",
      "\t quart, pint\n",
      "\n",
      "case.n.01 (2)\n",
      "\t piece, time\n",
      "\n",
      "force_unit.n.01 (2)\n",
      "\t pound\n",
      "\n",
      "united_states_liquid_unit.n.01 (2)\n",
      "\t quart, pint\n",
      "\n",
      "time.n.03 (2)\n",
      "\t day, piece\n",
      "\n",
      "soup.n.01 (2)\n",
      "\t broth\n",
      "\n",
      "attendant.n.01 (2)\n",
      "\t page\n",
      "\n",
      "part.n.09 (2)\n",
      "\t round, half\n",
      "\n",
      "thing.n.12 (2)\n",
      "\t water, piece\n",
      "\n",
      "british_capacity_unit.n.01 (2)\n",
      "\t quart, pint\n",
      "\n",
      "avoirdupois_unit.n.01 (2)\n",
      "\t pound, ounce\n",
      "\n",
      "happening.n.01 (2)\n",
      "\t gravy, fire\n",
      "\n",
      "share.n.01 (2)\n",
      "\t slice, piece\n",
      "\n",
      "vegetable.n.01 (2)\n",
      "\t onion, mushroom\n",
      "\n",
      "foodstuff.n.02 (2)\n",
      "\t egg, flour\n",
      "\n",
      "herb.n.01 (2)\n",
      "\t carrot, parsley\n",
      "\n",
      "element.n.05 (2)\n",
      "\t fire, water\n",
      "\n",
      "food.n.02 (2)\n",
      "\t butter, meat\n",
      "\n",
      "agaric.n.02 (2)\n",
      "\t mushroom\n",
      "\n",
      "distance.n.01 (2)\n",
      "\t piece, hour\n",
      "\n",
      "meat.n.01 (2)\n",
      "\t beef, mutton\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories_from_hypernyms(cookbook_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_period.n.01 (15)\n",
      "\t month, day, week, school, year, night, time\n",
      "\n",
      "building.n.01 (5)\n",
      "\t school, house, club\n",
      "\n",
      "administrative_district.n.01 (5)\n",
      "\t state, country, county, city\n",
      "\n",
      "unit.n.03 (5)\n",
      "\t member, home, family, house, company\n",
      "\n",
      "time_unit.n.01 (4)\n",
      "\t night, day, month\n",
      "\n",
      "educational_institution.n.01 (3)\n",
      "\t school, university\n",
      "\n",
      "compartment.n.02 (3)\n",
      "\t car\n",
      "\n",
      "body.n.02 (3)\n",
      "\t school, administration, university\n",
      "\n",
      "male.n.02 (3)\n",
      "\t man\n",
      "\n",
      "collection.n.01 (2)\n",
      "\t family, law\n",
      "\n",
      "association.n.01 (2)\n",
      "\t family, club\n",
      "\n",
      "activity.n.01 (2)\n",
      "\t service, game\n",
      "\n",
      "government.n.01 (2)\n",
      "\t state, court\n",
      "\n",
      "attribute.n.02 (2)\n",
      "\t state, time\n",
      "\n",
      "title.n.06 (2)\n",
      "\t mrs., mr.\n",
      "\n",
      "social_gathering.n.01 (2)\n",
      "\t company, meeting\n",
      "\n",
      "selling.n.01 (2)\n",
      "\t sale\n",
      "\n",
      "political_unit.n.01 (2)\n",
      "\t state, country\n",
      "\n",
      "system.n.04 (2)\n",
      "\t program, government\n",
      "\n",
      "social_control.n.01 (2)\n",
      "\t administration, government\n",
      "\n",
      "legal_document.n.01 (2)\n",
      "\t law, bill\n",
      "\n",
      "force.n.04 (2)\n",
      "\t service, law\n",
      "\n",
      "head_of_state.n.01 (2)\n",
      "\t president\n",
      "\n",
      "idea.n.01 (2)\n",
      "\t program, plan\n",
      "\n",
      "region.n.01 (2)\n",
      "\t house, county\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories_from_hypernyms(brown_news_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *** What works well? *** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** What are the problems, and how can this code be improved? ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
