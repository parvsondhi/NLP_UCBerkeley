{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DESCRPTION OF WHAT I HAVE DONE\n",
    "\n",
    "I have experiemented with a combination of two different types of algorithms. I will explain both.\n",
    "\n",
    "This python notebook performed better taking into account both of texts even though it was less complex. It uses a combination of chunking and unigram frequency. The steps I took are as follows:\n",
    "1. Read in file and tokenize by sentence and word using default tagger \n",
    "2. Normalize by only accepting letters (used regex), removing stop words, and also lemmatizing\n",
    "3. Use chunking on Noun Phrases\n",
    "4. Find 50 most common unigrams\n",
    "5. Select chunks by chunks which have the greatest number of unigrams\n",
    "\n",
    "***Issues:*** The main issue with this key phrase assignment is the weighting of the unigrams. Many common unigrams are not important to the text. I assumed that subsetting by 50 unigrams would normalize the result. However, many non-important unigrams dominate the subsetting of the chunkers which results in a skewed output. Tf-idf should have been used here\n",
    "\n",
    "I have also included a second notebook for reference of what I have experimented with. It uses a combination of bigram collocations and unigram frequency. The steps I took are as follows:\n",
    "1. Read in file and tokenize by sentence and word using default tagger \n",
    "2. Normalize by only accepting letters (used regex), removing stop words, and also lemmatizing\n",
    "3. Find bigram collocations\n",
    "4. Find 50 most common unigrams\n",
    "5. Find sentences which use the bigram collocations\n",
    "6. Select those subset of sentences by sentences which have the greatest number of unigrams\n",
    "\n",
    "***Issues:*** This method arguably worked better on my own text since the bigrams are high in content. However, it worked poorly on the 'news' corpus. The vast majority of bigram collocations were of very little importance. Also, the frequency threshold for my own text versus the 'news' corpus varied dramatically (ie a frequency of at least 10 collocations produced 15 bigrams for my own text but only 2 bigram for the 'news' corpus). Thus, too much exploratory effort on the user would be required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk, re, string\n",
    "import pandas as pd\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize\n",
    "#import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import brown\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global Variables**\n",
    "\n",
    "stopwords, lemmatizer, english_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english') + ['new', 'york', 'ye','o','thou','thus','hath','one', 'thee', 'thy','miss']\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**function to read in file**\n",
    "\n",
    "specific to Zarathustra (need to sepearate gutenberg text from book text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    with open(file, 'r') as f_in:\n",
    "        f_in = f_in.read()\n",
    "        f_in = f_in.strip().split()\n",
    "        f_in = \" \".join(f_in)\n",
    "    \n",
    "    # calculate the indices of the beginning and end of the book\n",
    "    #start = f_in.find('\"MIDDAY AND ETERNITY.\"')\n",
    "    #end = f_in.find(\"morning sun coming out of gloomy mountains.\")\n",
    "\n",
    "    # subset the text to those indices\n",
    "    #f_in = f_in[start:end]\n",
    "    \n",
    "    return f_in\n",
    "\n",
    "#read_file('Zarathustra.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** file reader which will read any url text **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_file_url(url):\n",
    "    with urllib.request.urlopen(link) as url:\n",
    "        f_in =  url.read().decode(url.headers.get_content_charset())\n",
    "        f_in = f_in.strip().split()\n",
    "        f_in = \" \".join(f_in)\n",
    "    return f_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**function to tokenize text by sentence then word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to tokenize text by sentence then word\n",
    "def tokenize_text(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences\n",
    "    raw_sents = [nltk.word_tokenize(word) for word in raw_sents]\n",
    "    return [nltk.pos_tag(word) for word in raw_sents]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get text for all words, real words, and not real words**\n",
    "\n",
    "only will use real_words (word:tag pair of any word that you can find in the english dicitonary) and real_just_words (extract only the words from real_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_text(_tokenized_text):\n",
    "    all_words, real_words, real_just_words, not_words = [], [], [], []\n",
    "    for sent in _tokenize_text:\n",
    "        temp_all, temp_real, temp_real_just, temp_not = [], [], [], []\n",
    "        for word in sent:\n",
    "            word_word = \"\".join(re.findall(r'[a-zA-Z]',word[0]))\n",
    "            if (word_word.lower()) not in stop_words and wordnet_lemmatizer.lemmatize(word_word.lower()) in english_vocab and len(word_word)>0:\n",
    "                temp_real.append((wordnet_lemmatizer.lemmatize(word_word.lower()),word[1]))\n",
    "            if len(word_word)>0:\n",
    "                temp_real_just.append(word)\n",
    "            if (word_word.lower()) not in stop_words and wordnet_lemmatizer.lemmatize(word_word.lower()) not in english_vocab and len(word_word)>0:\n",
    "                temp_not.append((wordnet_lemmatizer.lemmatize(word_word.lower()),word[1]))\n",
    "            if (word_word.lower()) not in stop_words and len(word_word)>0:\n",
    "                temp_all.append((wordnet_lemmatizer.lemmatize(word_word.lower()),word[1]))\n",
    "        all_words.append(temp_all)           \n",
    "        real_words.append(temp_real)\n",
    "        real_just_words.append(temp_real_just)\n",
    "        not_words.append(temp_not)\n",
    "    return all_words, real_words, real_just_words, not_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**chunk on NounPhrase**\n",
    "\n",
    "extract noun phrases from each sentence and create list of noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Noun_Phrase(real_just_words):\n",
    "    real_just_long = [word for sent in real_just_words for word in sent]\n",
    "\n",
    "    chunker_Zara = \"NounPhrase: {(<J.*>*<NN.*>+(<TO>*)(<CD>*)(<CC>*)(<DT>*)(<IN>*)(<POS>*)<J.*>*<NN.*>+)+}\"\n",
    "\n",
    "    cp = nltk.RegexpParser(chunker_Zara)\n",
    "    chunked = cp.parse(real_just_long)\n",
    "\n",
    "    # NOTE: created np code below with help from StackOverflow post\n",
    "    np = [' '.join([y[0] for y in x.leaves()]) for x in chunked.subtrees() if x.label() == \"NounPhrase\"]\n",
    "    return np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**find unigrams and get top 50 most frequent**\n",
    "\n",
    "exclude first 5 as they skew the outcomes in the subsequent subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def real_nouns_adj(real_words):\n",
    "    real_nouns_adj = []\n",
    "    for sent in real_words:\n",
    "        temp = []\n",
    "        for word in sent:\n",
    "            if re.findall(r'(^N.*|^J.*)',word[1]):\n",
    "                temp.append(word)\n",
    "        real_nouns_adj.append(temp)\n",
    "    return real_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def real_unigrams(real_nouns_adj):\n",
    "    real_unigrams = [word for sent in real_nouns_adj for word in sent]\n",
    "    \n",
    "    real_unigrams_freq = nltk.FreqDist(real_unigrams)\n",
    "    top_unigrams = real_unigrams_freq.most_common(100)\n",
    "    top_unigrams = top_unigrams[5:50]\n",
    "    return top_unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**subset chunk by count of freqeuent unigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunk_list(np, word_unigrams):\n",
    "    chunk_list = []\n",
    "    for index, chunk in enumerate(np):\n",
    "        #for item in colloc_list:\n",
    "        for item in real_unigrams:\n",
    "            #if item[0] in chunk and item[1] in chunk:\n",
    "            if item[0][0] in chunk:\n",
    "                temp_token = nltk.word_tokenize(chunk.lower())\n",
    "                count = 0\n",
    "                for word in temp_token:\n",
    "                    if word in word_unigrams:\n",
    "                        count+=1\n",
    "                chunk_list.append((chunk, count))\n",
    "    chunk_list = set(list(chunk_list))\n",
    "    return chunk_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Calling my functions individually caused errors... \n",
    "not sure why so\n",
    "I unfortunately had to wrap them all within one function called 'final_chunk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_chunk():\n",
    "    real_nouns_adj = []\n",
    "    for sent in real_words:\n",
    "        temp = []\n",
    "        for word in sent:\n",
    "            if re.findall(r'(^N.*|^J.*)',word[1]):\n",
    "                temp.append(word)\n",
    "        real_nouns_adj.append(temp) \n",
    "\n",
    "    real_unigrams = [word for sent in real_nouns_adj for word in sent]\n",
    "\n",
    "\n",
    "\n",
    "    real_unigrams_freq = nltk.FreqDist(real_unigrams)\n",
    "    top_unigrams = real_unigrams_freq.most_common(100)\n",
    "    top_unigrams = top_unigrams[5:50]\n",
    "\n",
    "    np = Noun_Phrase(real_just_words)\n",
    "    word_unigrams = [item[0][0] for item in top_unigrams]\n",
    "    \n",
    "    word_count_list = []\n",
    "    chunk_list = []\n",
    "    for index, chunk in enumerate(np):\n",
    "        #for item in colloc_list:\n",
    "        for item in top_unigrams:\n",
    "            #if item[0] in chunk and item[1] in chunk:\n",
    "            if item[0][0] in chunk:\n",
    "                temp_token = nltk.word_tokenize(chunk.lower())\n",
    "                count = 0\n",
    "                word_count = []\n",
    "                for word in temp_token:\n",
    "                    if word in word_unigrams:\n",
    "                        if word.lower() not in word_count:\n",
    "                            word_count.append(word.lower())\n",
    "                            count+=1\n",
    "                chunk_list.append((chunk, count))\n",
    "                word_count_list.append(word_count)\n",
    "    combined_list = list(zip(chunk_list,word_count_list))\n",
    "    #combined_list = list(set(combined_list))\n",
    "    return combined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def final_chunk_ordered(combined_list):\n",
    "#     sort_chunk_list = sorted(combined_list,key=lambda x: x[0][1], reverse=True)\n",
    "#     sort_chunk_list[:20]\n",
    "\n",
    "#     final_list = []\n",
    "#     for item in sort_chunk_list:\n",
    "#         final_list.append((item[0][0],item[0][1],tuple(set(item[1]))))\n",
    "\n",
    "#     final_tuple = set(tuple(final_list))\n",
    "#     sort_chunk_list = sorted(final_tuple,key=lambda x: x[1], reverse=True)\n",
    "#     sort_chunk_list = sort_chunk_list[:20]\n",
    "#     return sort_chunk_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------\n",
    "### Test Out different Texts\n",
    "----------------------------------------------------------------------------------------\n",
    "NOTE: again for some reason using a function caused errors... I had to repeat the cleaning of each chunk in order to use in panda table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk</th>\n",
       "      <th>Number Unigrams</th>\n",
       "      <th>Unigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>way with old people</td>\n",
       "      <td>3</td>\n",
       "      <td>(people, way, old)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>higher body shalt thou create a first movement</td>\n",
       "      <td>3</td>\n",
       "      <td>(body, higher, first)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sake of mine evil spirit.</td>\n",
       "      <td>3</td>\n",
       "      <td>(spirit, mine, evil)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>evil eye virtue. And</td>\n",
       "      <td>3</td>\n",
       "      <td>(virtue, eye, evil)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>old brown drop of golden happiness golden wine...</td>\n",
       "      <td>3</td>\n",
       "      <td>(something, happiness, old)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>friend the bad conscience art thou</td>\n",
       "      <td>3</td>\n",
       "      <td>(bad, friend, art)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mine evil spirit of deceit</td>\n",
       "      <td>3</td>\n",
       "      <td>(spirit, mine, evil)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>old woman Many fine things hath Zarathustra</td>\n",
       "      <td>3</td>\n",
       "      <td>(many, woman, old)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mine evil spirit</td>\n",
       "      <td>3</td>\n",
       "      <td>(spirit, mine, evil)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Project Gutenberg-tm electronic work</td>\n",
       "      <td>2</td>\n",
       "      <td>(project, work)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>heart after such happiness</td>\n",
       "      <td>2</td>\n",
       "      <td>(happiness, heart)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>own way mine ears</td>\n",
       "      <td>2</td>\n",
       "      <td>(mine, way)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ordereth this Ah mine angry mistress wisheth</td>\n",
       "      <td>2</td>\n",
       "      <td>(mine, ah)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>work and the Project Gutenberg-tm trademark</td>\n",
       "      <td>2</td>\n",
       "      <td>(project, work)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>long populace-ears Ye higher men learn</td>\n",
       "      <td>2</td>\n",
       "      <td>(higher, long)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eye of happiness</td>\n",
       "      <td>2</td>\n",
       "      <td>(eye, happiness)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>woman thy little truth</td>\n",
       "      <td>2</td>\n",
       "      <td>(woman, truth)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>last mine abyss</td>\n",
       "      <td>2</td>\n",
       "      <td>(mine, last)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>life O happiness before eventide O</td>\n",
       "      <td>2</td>\n",
       "      <td>(life, happiness)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>world of happiness</td>\n",
       "      <td>2</td>\n",
       "      <td>(world, happiness)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Chunk  Number Unigrams  \\\n",
       "0                                 way with old people                3   \n",
       "1      higher body shalt thou create a first movement                3   \n",
       "2                           sake of mine evil spirit.                3   \n",
       "3                                evil eye virtue. And                3   \n",
       "4   old brown drop of golden happiness golden wine...                3   \n",
       "5                  friend the bad conscience art thou                3   \n",
       "6                          mine evil spirit of deceit                3   \n",
       "7         old woman Many fine things hath Zarathustra                3   \n",
       "8                                    mine evil spirit                3   \n",
       "9                Project Gutenberg-tm electronic work                2   \n",
       "10                         heart after such happiness                2   \n",
       "11                                  own way mine ears                2   \n",
       "12       ordereth this Ah mine angry mistress wisheth                2   \n",
       "13        work and the Project Gutenberg-tm trademark                2   \n",
       "14             long populace-ears Ye higher men learn                2   \n",
       "15                                   eye of happiness                2   \n",
       "16                             woman thy little truth                2   \n",
       "17                                    last mine abyss                2   \n",
       "18                 life O happiness before eventide O                2   \n",
       "19                                 world of happiness                2   \n",
       "\n",
       "                       Unigrams  \n",
       "0            (people, way, old)  \n",
       "1         (body, higher, first)  \n",
       "2          (spirit, mine, evil)  \n",
       "3           (virtue, eye, evil)  \n",
       "4   (something, happiness, old)  \n",
       "5            (bad, friend, art)  \n",
       "6          (spirit, mine, evil)  \n",
       "7            (many, woman, old)  \n",
       "8          (spirit, mine, evil)  \n",
       "9               (project, work)  \n",
       "10           (happiness, heart)  \n",
       "11                  (mine, way)  \n",
       "12                   (mine, ah)  \n",
       "13              (project, work)  \n",
       "14               (higher, long)  \n",
       "15             (eye, happiness)  \n",
       "16               (woman, truth)  \n",
       "17                 (mine, last)  \n",
       "18            (life, happiness)  \n",
       "19           (world, happiness)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################\n",
    "###### My Text: Thus Spoke Zarathustra ######\n",
    "#############################################\n",
    "\n",
    "_read_file = read_file('Zarathustra.txt')\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "# call appropriate functions\n",
    "_tokenize_text = tokenize_text(_read_file)\n",
    "all_words, real_words, real_just_words, not_words = all_text(_tokenize_text)\n",
    "combined_list = final_chunk()\n",
    "sort_chunk_list = sorted(combined_list,key=lambda x: x[0][1], reverse=True)\n",
    "sort_chunk_list[:20]\n",
    "\n",
    "# clean the output of sort_chunk_list and put into tuple of tuples\n",
    "final_list = []\n",
    "for item in sort_chunk_list:\n",
    "    final_list.append((item[0][0],item[0][1],tuple(set(item[1]))))\n",
    "\n",
    "# get unique tuples, sort by number of unigrams, then display in panda DataFrame\n",
    "final_tuple = set(tuple(final_list))\n",
    "sort_chunk_list = sorted(final_tuple,key=lambda x: x[1], reverse=True)\n",
    "sort_chunk_list = sort_chunk_list[:20]\n",
    "#final_chunk_ordered = final_chunk_ordered(combined_list)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "df = pd.DataFrame(sort_chunk_list, columns=['Chunk', 'Number Unigrams', 'Unigrams'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bigram/collocation methodology from the previous workbook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "higher men\n",
    "['higher']\n",
    "Ye creating ones ye higher men\n",
    "\n",
    "\n",
    "good evil\n",
    "['evil']\n",
    "But the state lieth in all languages of good and evil and whatever it saith it lieth and whatever it hath it hath stolen\n",
    "\n",
    "\n",
    "good bad\n",
    "['life', 'bad']\n",
    "The state I call it where all are poison-drinkers the good and the bad the state where all lose themselves the good and the bad the state where the slow suicide of all is called life\n",
    "\n",
    "\n",
    "old man\n",
    "['bad', 'old']\n",
    "An old man appeared who carried a light and asked Who cometh unto me and my bad sleep\n",
    "\n",
    "\n",
    "spirit gravity\n",
    "['spirit']\n",
    "Upwards in spite of the spirit that drew it downwards towards the abyss the spirit of gravity my devil and arch-enemy\n",
    "\n",
    "\n",
    "old magician\n",
    "['best', 'thing', 'bad', 'old']\n",
    "Thou bad old magician THAT is the best and the honestest thing I honour in thee that thou hast become weary of thyself and hast expressed it 'I am not great\n",
    "\n",
    "\n",
    "higher man\n",
    "['higher']\n",
    "The higher man\n",
    "\n",
    "\n",
    "mine animal\n",
    "['mine']\n",
    "O mine animals are ye also cruel\n",
    "\n",
    "\n",
    "voluntary beggar\n",
    "['art', 'heart']\n",
    "Art thou not the voluntary beggar who once cast away great riches Who was ashamed of his riches and of the rich and fled to the poorest to bestow upon them his abundance and his heart\n",
    "\n",
    "\n",
    "mine enemy\n",
    "['mine']\n",
    "Even mine enemies pertain to my bliss\n",
    "\n",
    "\n",
    "many people\n",
    "['earth', 'bad', 'many']\n",
    "Many lands saw Zarathustra and many peoples no greater power did Zarathustra find on earth than the creations of the loving ones good and bad are they called\n",
    "\n",
    "\n",
    "man something\n",
    "[]\n",
    "All beings hitherto have created something beyond themselves and ye want to be the ebb of that great tide and would rather go back to the beast than surpass man\n",
    "\n",
    "\n",
    "right time\n",
    "['time']\n",
    "Yet strange soundeth the precept Die at the right time\n",
    "\n",
    "\n",
    "highest hope\n",
    "['animal', 'new']\n",
    "And it is the great noontide when man is in the middle of his course between animal and Superman and celebrateth his advance to the evening as his highest hope for it is the advance to a new morning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk</th>\n",
       "      <th>Number Unigrams</th>\n",
       "      <th>Unigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>statistics last week Texas 545-yard spree agai...</td>\n",
       "      <td>4</td>\n",
       "      <td>(week, state, texas, washington)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>junior high school system or the system Board ...</td>\n",
       "      <td>4</td>\n",
       "      <td>(high, school, system, board)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>time a board member</td>\n",
       "      <td>3</td>\n",
       "      <td>(member, board, time)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>defendants Washington July President Kennedy t...</td>\n",
       "      <td>3</td>\n",
       "      <td>(president, today, washington)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kathleen Mason Jefferson high school Phil Reif...</td>\n",
       "      <td>3</td>\n",
       "      <td>(high, school, washington)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>night game and a single test Sunday afternoon ...</td>\n",
       "      <td>3</td>\n",
       "      <td>(game, night, sunday)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>higher area tax The Washington state supreme c...</td>\n",
       "      <td>3</td>\n",
       "      <td>(state, tax, washington)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Washington Feb. President Kennedy today</td>\n",
       "      <td>3</td>\n",
       "      <td>(president, today, washington)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Monday 's special North Providence Town Counci...</td>\n",
       "      <td>3</td>\n",
       "      <td>(monday, president, meeting)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tomorrow night to the American people</td>\n",
       "      <td>3</td>\n",
       "      <td>(night, people, american)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yesterday for Sunday night</td>\n",
       "      <td>3</td>\n",
       "      <td>(yesterday, night, sunday)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Monday at City Hall F. Morris Cochran universi...</td>\n",
       "      <td>3</td>\n",
       "      <td>(monday, president, city)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wilson High School and president</td>\n",
       "      <td>3</td>\n",
       "      <td>(high, school, president)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>American people as president</td>\n",
       "      <td>3</td>\n",
       "      <td>(president, people, american)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Washington-Oregon football game Beaverton Scho...</td>\n",
       "      <td>3</td>\n",
       "      <td>(game, school, board)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Washington State game</td>\n",
       "      <td>3</td>\n",
       "      <td>(game, state, washington)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>state sales tax from per cent</td>\n",
       "      <td>3</td>\n",
       "      <td>(state, tax, cent)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lunch hour yesterday at Fuhrmann Junior High S...</td>\n",
       "      <td>3</td>\n",
       "      <td>(high, school, yesterday)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>senior high school teaching certificate A norm...</td>\n",
       "      <td>3</td>\n",
       "      <td>(high, school, year)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>concert tomorrow night for Puerto Rico Governo...</td>\n",
       "      <td>2</td>\n",
       "      <td>(president, night)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Chunk  Number Unigrams  \\\n",
       "0   statistics last week Texas 545-yard spree agai...                4   \n",
       "1   junior high school system or the system Board ...                4   \n",
       "2                                 time a board member                3   \n",
       "3   defendants Washington July President Kennedy t...                3   \n",
       "4   Kathleen Mason Jefferson high school Phil Reif...                3   \n",
       "5   night game and a single test Sunday afternoon ...                3   \n",
       "6   higher area tax The Washington state supreme c...                3   \n",
       "7             Washington Feb. President Kennedy today                3   \n",
       "8   Monday 's special North Providence Town Counci...                3   \n",
       "9               tomorrow night to the American people                3   \n",
       "10                         yesterday for Sunday night                3   \n",
       "11  Monday at City Hall F. Morris Cochran universi...                3   \n",
       "12                   Wilson High School and president                3   \n",
       "13                       American people as president                3   \n",
       "14  Washington-Oregon football game Beaverton Scho...                3   \n",
       "15                              Washington State game                3   \n",
       "16                      state sales tax from per cent                3   \n",
       "17  lunch hour yesterday at Fuhrmann Junior High S...                3   \n",
       "18  senior high school teaching certificate A norm...                3   \n",
       "19  concert tomorrow night for Puerto Rico Governo...                2   \n",
       "\n",
       "                            Unigrams  \n",
       "0   (week, state, texas, washington)  \n",
       "1      (high, school, system, board)  \n",
       "2              (member, board, time)  \n",
       "3     (president, today, washington)  \n",
       "4         (high, school, washington)  \n",
       "5              (game, night, sunday)  \n",
       "6           (state, tax, washington)  \n",
       "7     (president, today, washington)  \n",
       "8       (monday, president, meeting)  \n",
       "9          (night, people, american)  \n",
       "10        (yesterday, night, sunday)  \n",
       "11         (monday, president, city)  \n",
       "12         (high, school, president)  \n",
       "13     (president, people, american)  \n",
       "14             (game, school, board)  \n",
       "15         (game, state, washington)  \n",
       "16                (state, tax, cent)  \n",
       "17         (high, school, yesterday)  \n",
       "18              (high, school, year)  \n",
       "19                (president, night)  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################\n",
    "######### News Test from Brown Corpus #######\n",
    "#############################################\n",
    "\n",
    "_read_file = \" \".join([word for sent in brown.sents(categories='news') for word in sent])\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "# call appropriate functions\n",
    "_tokenize_text = tokenize_text(_read_file)\n",
    "all_words, real_words, real_just_words, not_words = all_text(_tokenize_text)\n",
    "combined_list = final_chunk()\n",
    "sort_chunk_list = sorted(combined_list,key=lambda x: x[0][1], reverse=True)\n",
    "sort_chunk_list[:20]\n",
    "\n",
    "# clean the output of sort_chunk_list and put into tuple of tuples\n",
    "final_list = []\n",
    "for item in sort_chunk_list:\n",
    "    final_list.append((item[0][0],item[0][1],tuple(set(item[1]))))\n",
    "\n",
    "# get unique tuples, sort by number of unigrams, then display in panda DataFrame\n",
    "final_tuple = set(tuple(final_list))\n",
    "sort_chunk_list = sorted(final_tuple,key=lambda x: x[1], reverse=True)\n",
    "sort_chunk_list = sort_chunk_list[:20]\n",
    "#final_chunk_ordered = final_chunk_ordered(combined_list)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "df = pd.DataFrame(sort_chunk_list, columns=['Chunk', 'Number Unigrams', 'Unigrams'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "######### INPUT YOUR OWN FILE IN file #######\n",
    "#############################################\n",
    "\n",
    "file = \"input file here just as seen below\"\n",
    "\n",
    "#file = \"http://people.ischool.berkeley.edu/~tygar/for.i206/pg1342.txt\"\n",
    "\n",
    "##############################\n",
    "# UNCOMMENT EVERYTHING BELOW #\n",
    "##############################\n",
    "\n",
    "# _read_file = read_file_url(file)\n",
    "\n",
    "# ##########################################################################################\n",
    "\n",
    "# # call appropriate functions\n",
    "# _tokenize_text = tokenize_text(_read_file)\n",
    "# all_words, real_words, real_just_words, not_words = all_text(_tokenize_text)\n",
    "# combined_list = final_chunk()\n",
    "# sort_chunk_list = sorted(combined_list,key=lambda x: x[0][1], reverse=True)\n",
    "# sort_chunk_list[:20]\n",
    "\n",
    "# # clean the output of sort_chunk_list and put into tuple of tuples\n",
    "# final_list = []\n",
    "# for item in sort_chunk_list:\n",
    "#     final_list.append((item[0][0],item[0][1],tuple(set(item[1]))))\n",
    "\n",
    "# # get unique tuples, sort by number of unigrams, then display in panda DataFrame\n",
    "# final_tuple = set(tuple(final_list))\n",
    "# sort_chunk_list = sorted(final_tuple,key=lambda x: x[1], reverse=True)\n",
    "# sort_chunk_list = sort_chunk_list[:20]\n",
    "# #final_chunk_ordered = final_chunk_ordered(combined_list)\n",
    "# pd.set_option('display.expand_frame_repr', True)\n",
    "# df = pd.DataFrame(sort_chunk_list, columns=['Chunk', 'Number Unigrams', 'Unigrams'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk</th>\n",
       "      <th>Number Unigrams</th>\n",
       "      <th>Unigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>young mule That time</td>\n",
       "      <td>3</td>\n",
       "      <td>(time, young, mule)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle The big serious old brown bear</td>\n",
       "      <td>3</td>\n",
       "      <td>(old, jungle, big)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monkey People The next thing</td>\n",
       "      <td>3</td>\n",
       "      <td>(monkey, people, thing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seal people to a quiet place</td>\n",
       "      <td>3</td>\n",
       "      <td>(place, seal, people)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>end O bush-tailed thieves Mother Wolf</td>\n",
       "      <td>3</td>\n",
       "      <td>(end, wolf, mother)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day a white seal</td>\n",
       "      <td>3</td>\n",
       "      <td>(day, seal, white)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>foot to the ground Teddy</td>\n",
       "      <td>3</td>\n",
       "      <td>(ground, foot, teddy)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>back Little Brother</td>\n",
       "      <td>3</td>\n",
       "      <td>(little, back, brother)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>old Hay-bales The proper way</td>\n",
       "      <td>2</td>\n",
       "      <td>(way, old)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Project Gutenberg-tm electronic work</td>\n",
       "      <td>2</td>\n",
       "      <td>(project, work)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jungle till the eyes</td>\n",
       "      <td>2</td>\n",
       "      <td>(till, jungle)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>young mule Well</td>\n",
       "      <td>2</td>\n",
       "      <td>(young, mule)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>day 's work</td>\n",
       "      <td>2</td>\n",
       "      <td>(day, work)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>big huqas the water-pipes till</td>\n",
       "      <td>2</td>\n",
       "      <td>(till, big)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>first time a seal</td>\n",
       "      <td>2</td>\n",
       "      <td>(seal, time)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sea cow or a seal</td>\n",
       "      <td>2</td>\n",
       "      <td>(sea, seal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cud but the young mule</td>\n",
       "      <td>2</td>\n",
       "      <td>(young, mule)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>elephants all round Kala Nag</td>\n",
       "      <td>2</td>\n",
       "      <td>(kala, round)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>long night and the favor</td>\n",
       "      <td>2</td>\n",
       "      <td>(night, long)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>piece of good elephant stuff</td>\n",
       "      <td>2</td>\n",
       "      <td>(good, elephant)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Chunk  Number Unigrams  \\\n",
       "0                    young mule That time                3   \n",
       "1   Jungle The big serious old brown bear                3   \n",
       "2            Monkey People The next thing                3   \n",
       "3            seal people to a quiet place                3   \n",
       "4   end O bush-tailed thieves Mother Wolf                3   \n",
       "5                        day a white seal                3   \n",
       "6                foot to the ground Teddy                3   \n",
       "7                     back Little Brother                3   \n",
       "8            old Hay-bales The proper way                2   \n",
       "9    Project Gutenberg-tm electronic work                2   \n",
       "10                   jungle till the eyes                2   \n",
       "11                        young mule Well                2   \n",
       "12                            day 's work                2   \n",
       "13         big huqas the water-pipes till                2   \n",
       "14                      first time a seal                2   \n",
       "15                      sea cow or a seal                2   \n",
       "16                 cud but the young mule                2   \n",
       "17           elephants all round Kala Nag                2   \n",
       "18               long night and the favor                2   \n",
       "19           piece of good elephant stuff                2   \n",
       "\n",
       "                   Unigrams  \n",
       "0       (time, young, mule)  \n",
       "1        (old, jungle, big)  \n",
       "2   (monkey, people, thing)  \n",
       "3     (place, seal, people)  \n",
       "4       (end, wolf, mother)  \n",
       "5        (day, seal, white)  \n",
       "6     (ground, foot, teddy)  \n",
       "7   (little, back, brother)  \n",
       "8                (way, old)  \n",
       "9           (project, work)  \n",
       "10           (till, jungle)  \n",
       "11            (young, mule)  \n",
       "12              (day, work)  \n",
       "13              (till, big)  \n",
       "14             (seal, time)  \n",
       "15              (sea, seal)  \n",
       "16            (young, mule)  \n",
       "17            (kala, round)  \n",
       "18            (night, long)  \n",
       "19         (good, elephant)  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################\n",
    "###### My Text: Thus Spoke Zarathustra ######\n",
    "#############################################\n",
    "\n",
    "_read_file = read_file('mystery_text_narrative.txt')\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "# call appropriate functions\n",
    "_tokenize_text = tokenize_text(_read_file)\n",
    "all_words, real_words, real_just_words, not_words = all_text(_tokenize_text)\n",
    "combined_list = final_chunk()\n",
    "sort_chunk_list = sorted(combined_list,key=lambda x: x[0][1], reverse=True)\n",
    "sort_chunk_list[:20]\n",
    "\n",
    "# clean the output of sort_chunk_list and put into tuple of tuples\n",
    "final_list = []\n",
    "for item in sort_chunk_list:\n",
    "    final_list.append((item[0][0],item[0][1],tuple(set(item[1]))))\n",
    "\n",
    "# get unique tuples, sort by number of unigrams, then display in panda DataFrame\n",
    "final_tuple = set(tuple(final_list))\n",
    "sort_chunk_list = sorted(final_tuple,key=lambda x: x[1], reverse=True)\n",
    "sort_chunk_list = sort_chunk_list[:20]\n",
    "#final_chunk_ordered = final_chunk_ordered(combined_list)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "df = pd.DataFrame(sort_chunk_list, columns=['Chunk', 'Number Unigrams', 'Unigrams'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('young mule That time', 3, ('time', 'young', 'mule')),\n",
       " ('Jungle The big serious old brown bear', 3, ('old', 'jungle', 'big')),\n",
       " ('Monkey People The next thing', 3, ('monkey', 'people', 'thing')),\n",
       " ('seal people to a quiet place', 3, ('place', 'seal', 'people')),\n",
       " ('end O bush-tailed thieves Mother Wolf', 3, ('end', 'wolf', 'mother')),\n",
       " ('day a white seal', 3, ('day', 'seal', 'white')),\n",
       " ('foot to the ground Teddy', 3, ('ground', 'foot', 'teddy')),\n",
       " ('back Little Brother', 3, ('little', 'back', 'brother')),\n",
       " ('old Hay-bales The proper way', 2, ('way', 'old')),\n",
       " ('Project Gutenberg-tm electronic work', 2, ('project', 'work')),\n",
       " ('jungle till the eyes', 2, ('till', 'jungle')),\n",
       " ('young mule Well', 2, ('young', 'mule')),\n",
       " (\"day 's work\", 2, ('day', 'work')),\n",
       " ('big huqas the water-pipes till', 2, ('till', 'big')),\n",
       " ('first time a seal', 2, ('seal', 'time')),\n",
       " ('sea cow or a seal', 2, ('sea', 'seal')),\n",
       " ('cud but the young mule', 2, ('young', 'mule')),\n",
       " ('elephants all round Kala Nag', 2, ('kala', 'round')),\n",
       " ('long night and the favor', 2, ('night', 'long')),\n",
       " ('piece of good elephant stuff', 2, ('good', 'elephant'))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
