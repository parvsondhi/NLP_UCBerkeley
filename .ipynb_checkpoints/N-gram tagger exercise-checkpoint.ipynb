{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##A Simple Baseline Tagger##\n",
    "Keep in mind that the brown corpus is already tagged.  The simplest possible tagger assigns the **most likely** tag to each token. This establishes a baseline tagger.  So let's use the data we have to figure out what the most likely tag for English is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_likely_brown_tag():\n",
    "    tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
    "    return (nltk.FreqDist(tags).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use FreqDist and max() to find out which tag is the **most likely tag** for English according to the Brown corpus by counting how many tags have been assigned to the words in this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_likely_brown_tag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know empirically which is the most likely tag for English, we can make a baseline tagger that automatically assigns the most likely tag when we don't know what else to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('what', 'NN'), ('will', 'NN'), ('this', 'NN'), ('silly', 'NN'), ('tagger', 'NN'), ('do', 'NN'), ('?', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "def make_default_tagger(init_tag = 'NN'):\n",
    "    return(nltk.DefaultTagger(init_tag))\n",
    "    \n",
    "def run_tagger_on_sentence(tagger, sent):\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    print(tagger.tag(tokens))\n",
    "    \n",
    "sent = r'''what will this silly tagger do?'''\n",
    "run_tagger_on_sentence(make_default_tagger(),sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Train a Unigram Tagger From Pre-Tagged Text##\n",
    "Now train a unigram tagger on the news portion of the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_tagger_on_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d4e784ffe8de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munigram_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrun_tagger_on_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_unigram_tagger_on_brown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'run_tagger_on_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "def train_unigram_tagger(training_sents):\n",
    "    return(nltk.UnigramTagger(training_sents))\n",
    "\n",
    "# train on brown news\n",
    "def train_unigram_tagger_on_brown():\n",
    "    brown_train_sents = brown.tagged_sents(categories='news')\n",
    "    unigram_tagger = train_unigram_tagger(brown_train_sents)\n",
    "    return(unigram_tagger)\n",
    "\n",
    "run_tagger_on_sentence(train_unigram_tagger_on_brown(), sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Separate Training From Testing Data###\n",
    "But really we need to separate training and testing data.  We can use the handy python string slicing operator to do this *really* easily.  Here we divide into 90% training and 10% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('what', 'WDT'), ('will', 'MD'), ('this', 'DT'), ('silly', 'JJ'), ('tagger', None), ('do', 'DO'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "def create_data_sets(tagged_sents):\n",
    "    size = int(len(tagged_sents) * 0.9)\n",
    "    train_sents = tagged_sents[:size]\n",
    "    test_sents = tagged_sents[size:]\n",
    "    return train_sents, test_sents\n",
    "\n",
    "def train_and_test_unigram_tagger_on_brown():\n",
    "    sample_sents = brown.tagged_sents(categories='news')\n",
    "    brown_train_sents, brown_test_sents = create_data_sets(sample_sents)\n",
    "    unigram_tagger = train_unigram_tagger(brown_train_sents)\n",
    "    return(unigram_tagger, brown_train_sents, brown_test_sents)\n",
    "\n",
    "\n",
    "ut, brown_train_sents, brown_test_sents = train_and_test_unigram_tagger_on_brown()\n",
    "run_tagger_on_sentence(ut, sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Evaluation Metric###\n",
    "NLTK's tagger has a handy evaluation function built right in!  It automatically compares the output of your tagger with the tags assigned to the Brown corpus.  The score shown below is the average across the entire test collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812\n"
     ]
    }
   ],
   "source": [
    "print (\"%0.3f\" % ut.evaluate(brown_test_sents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Question###\n",
    "What is this evaluation metric measuring?\n",
    "* Answer: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which tags did it get wrong?  If you want to see what the gold standard tags were vs. what the tagger produced, here is some code to do it (written by Jason Ost, MIMS from 2014).  The first column is the word, the second is the tag from the gold standard, and the third is what the algorithm assigned.  (The last element is a little tricky: the tagger's tag() function expects a list of words as input, so you have to enclose \"w\" in square brackets, and it returns a list of tagged words (as two-element tuples), so you have to grab the second element of the first tuple, which is the predicted tag.  This works because the unigram tagger looks at each word in isolation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('For', 'IN', 'IN'),\n",
       " ('18', 'CD', 'CD'),\n",
       " ('months', 'NNS', 'NNS'),\n",
       " (',', ',', ','),\n",
       " ('Hamilton', 'NP', None),\n",
       " ('Holmes', 'NP', None),\n",
       " (',', ',', ','),\n",
       " ('19', 'CD', 'CD'),\n",
       " (',', ',', ','),\n",
       " ('and', 'CC', 'CC'),\n",
       " ('Charlayne', 'NP', None),\n",
       " ('Hunter', 'NP', 'NP-TL'),\n",
       " (',', ',', ','),\n",
       " ('18', 'CD', 'CD'),\n",
       " (',', ',', ','),\n",
       " ('had', 'HVD', 'HVD'),\n",
       " ('tried', 'VBN', 'VBN'),\n",
       " ('to', 'TO', 'TO'),\n",
       " ('get', 'VB', 'VB'),\n",
       " ('into', 'IN', 'IN'),\n",
       " ('the', 'AT', 'AT'),\n",
       " ('university', 'NN', 'NN'),\n",
       " ('.', '.', '.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_unigram_tagger_output (tagger, test_sent):\n",
    "   return ( [(w, t, tagger.tag([w])[0][1]) for w, t in test_sent])   \n",
    "    \n",
    "evaluate_unigram_tagger_output(ut, brown_test_sents[3])    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the another sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('``', '``', '``'),\n",
       " ('Surprised', 'VBN', None),\n",
       " ('and', 'CC', 'CC'),\n",
       " ('pleased', 'VBN', 'VBN'),\n",
       " (\"''\", \"''\", \"''\"),\n",
       " (',', ',', ','),\n",
       " ('Students', 'NNS', 'NNS'),\n",
       " ('Holmes', 'NP', None),\n",
       " ('and', 'CC', 'CC'),\n",
       " ('Hunter', 'NP', 'NP-TL'),\n",
       " ('may', 'MD', 'MD'),\n",
       " ('enter', 'VB', 'VB'),\n",
       " ('the', 'AT', 'AT'),\n",
       " ('University', 'NN-TL', 'NN-TL'),\n",
       " ('of', 'IN-TL', 'IN'),\n",
       " ('Georgia', 'NP-TL', 'NP'),\n",
       " ('this', 'DT', 'DT'),\n",
       " ('week', 'NN', 'NN'),\n",
       " ('.', '.', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_unigram_tagger_output(ut, brown_test_sents[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Train an N-Gram Tagger With Backoff ##\n",
    "\n",
    "Below is code for a bigram tagger with backoff.  When it encounters a token, it first\n",
    "1. Tries tagging the token with the bigram tagger.\n",
    "2. If the bigram tagger is unable to find a tag for the token, tries the unigram tagger.\n",
    "3. If the unigram tagger is also unable to find a tag, uses the default tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.845\n"
     ]
    }
   ],
   "source": [
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "ngram_tagger = build_backoff_tagger(brown_train_sents)\n",
    "bigram_tagger = ngram_tagger\n",
    "print (\"%0.3f\" % ngram_tagger.evaluate(brown_test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Train and Evaluate a Trigram Tagger ##\n",
    "\n",
    "Modify build_backoff_tagger() to build a backoff trigram tagger.  Evaluate the results.  How does it do compared to the previous backoff tagger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843\n"
     ]
    }
   ],
   "source": [
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    t3 = nltk.TrigramTagger(train_sents, backoff=t2)\n",
    "    return t3\n",
    "ngram_tagger = build_backoff_tagger(brown_train_sents)\n",
    "trigram_tagger = ngram_tagger\n",
    "print (\"%0.3f\" % ngram_tagger.evaluate(brown_test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Train a Simplified Tagger ##\n",
    "Train and evaluate a bigram backoff tagger like the one above but using the universal Brown tagset (or make a tagset of your own by discarding all but the first character of each tag name). This tagger has fewer distinctions to make but more ambiguity.  Evaluate its performance.  How does it compare to the previous tagger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a Tagger by Looking at Tags that Follow Tags ##\n",
    "(For this exercise, use your regular tagger, not the simplified one.)  The word **to** is frequently confused; it can be helpful to inspect the context it occurs in.  This code shows how to view the frequency of the tag that *follows* the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def examine_tag_contexts(tagger, target_word, target_tag):\n",
    "    test_sents = [tagger.tag(sent) for sent in brown.sents(categories='editorial')]\n",
    "    tags = [b[1] for test_sent in test_sents \n",
    "            for (a,b) in nltk.bigrams(test_sent)\n",
    "            if a[0] == target_word and a[1] == target_tag]\n",
    "    fd = nltk.FreqDist(tags)\n",
    "    print (\"Tags that follow the target word and tag \" + target_word + \" and \" + target_tag)\n",
    "    fd.tabulate(15)\n",
    "examine_tag_contexts(ngram_tagger, 'to', 'TO')\n",
    "examine_tag_contexts(ngram_tagger, 'to', 'IN')                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
