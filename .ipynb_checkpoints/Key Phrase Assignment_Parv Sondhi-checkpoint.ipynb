{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Steps\n",
    "\n",
    "The Algorithm uses a combination of chunking, Trigram Collocation and Wordnet Hypernyms to help gather relevant information regarding the text. \n",
    "\n",
    "The steps I took are as follows:\n",
    "\n",
    "1. Read in file and tokenize by sentence and word using train backoff tagger\n",
    "2. Normalize by only accepting letters (used regex), removing stop words\n",
    "3. Use collocation algorithms to find frequently occuring Bigrams and Trigrams\n",
    "4. Remove all Bigrams with verb occurrences to narrow BiGram set \n",
    "5. Order the Bigrams and Trigrams using PMI, and Chi-Square\n",
    "6. Use chunking on Noun Phrases based on a defined grammar to extract key chunks of information\n",
    "7. Extract most common Hyperterms of the most common unigrams occurring in the text\n",
    "8. Compare and Contrast different results to establish best Gist result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Statements to gather relevant packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk, re, string\n",
    "import pandas as pd\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.util import ngrams\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import brown\n",
    "import urllib.request\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grammar Rule to be used for chunking**\n",
    "\n",
    "As we see, before arriving at this Grammar rule, I worked through multiple grammar rules to help develop a relevant noun phrase chunker which would yield the most appropriate and relevant results for chunking of the text\n",
    "\n",
    "Some example grammar rules tried:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NounPhrase: \\{\\<J\\.\\*>\\<N.*>}\n",
    "# NounPhrase: {(<J.*>*<N.*>+(<CD>*)(<CC>*)(<DT>*)(<IN>*)(<POS>*)<J.*>*<N.*>+)}\n",
    "# NounPhrase: {((<DT>*)(<POS>*)(<IN>*)(<FW.*>*)<J.*>*<NN.*>+)+}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "            NounPhrase: {((<DT>*)(<POS>*)(<FW.*>*)<J.*>*<N.*>+)+}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in the text file from the local user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readtextfile(file):\n",
    "    with open(file) as w:\n",
    "        text = w.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read URL file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file_url(url):\n",
    "    with urllib.request.urlopen(url) as url:\n",
    "        f_in =  url.read().decode(url.headers.get_content_charset())\n",
    "        f_in = f_in.strip().split()\n",
    "        f_in = \" \".join(f_in)\n",
    "    return f_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing of the raw text**\n",
    "\n",
    "This text processing is specific to the Donal Trump Speeches file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_preprocessing_trumpspeeches(text):\n",
    "    text = text.replace('\\ufeff', '')\n",
    "    new_text = re.sub('[\\n]+','\\n', text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My function to use the above created grammar rule to parse through the text and extract relevant chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_chunker(sent_list):\n",
    "    mychunks = []\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    for sentence in sent_list:\n",
    "        result = cp.parse(sentence)\n",
    "        mychunks.append(result)\n",
    "    return mychunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Word Tokenizer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "    pattern = r'''(?x)  # set flag to allow verbose regexps\n",
    "     (?:[A-Z]\\.)+[A-Z]*        # abbreviations, e.g. U.S.A.\n",
    "    | [a-zA-Z]+(?:[-'][a-zA-Z]+)*            # words with optional internal hyphens or apostrophes         \n",
    "    | \\$?\\d+(?:\\.\\d+)?%?     # currency (dollars only, e.g. $12.40, $33, $.9) and digits \n",
    "    | [+/\\-@&*.,;\"'?():\\-_`] #special symbols\n",
    "    '''\n",
    "    \n",
    "    tokens = nltk.regexp_tokenize(text,pattern)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sentence Tokenizer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_tokenizer(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus)\n",
    "    return [word_tokenizer(item) for item in raw_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Normalization **\n",
    "\n",
    "This function will remove any stop words and punctuations from the text that is sent to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    sentence = [item for item in text if item[0] not in punctuations and item[0].lower() not in stopwords.words('english')]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Function to tag the text using the standard POS Tagger **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standard_pos_tagger(text):\n",
    "    if(isinstance(text[0],list)):\n",
    "        tagged_POS = [nltk.pos_tag(sent) for sent in text]\n",
    "    else:\n",
    "        tagged_POS = nltk.pos_tag(text)\n",
    "    return tagged_POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Function to implement the backoff tagger based on the brown tag set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data_sets(sentences):\n",
    "    size = int(len(sentences) * 0.9)\n",
    "    train_sents = sentences[:size]\n",
    "    test_sents = sentences[size:]\n",
    "    return train_sents, test_sents\n",
    "\n",
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    t3 = nltk.TrigramTagger(train_sents, backoff=t2)\n",
    "    return t3\n",
    "\n",
    "\n",
    "def train_tagger(already_tagged_sents):\n",
    "    train_sents, test_sents = create_data_sets(already_tagged_sents)\n",
    "    ngram_tagger = build_backoff_tagger(train_sents)\n",
    "    print (\"%0.3f pos accuracy on test set\" % ngram_tagger.evaluate(test_sents))\n",
    "    return ngram_tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training the tagger on brown tagset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_tagger_on_brown():\n",
    "    modified_speech_sents = [[('common', 'JJ'), ('Hard-working', 'JJ'), ('people', 'NNS'), ('.', '.')],\n",
    "                        [(\"I'm\", 'PPSS+BEM'), ('a', 'AT'), ('Republican', 'NP'), ('.', '.')],\n",
    "                        [(\"I'm\", 'PPSS+BEM'), ('Republican', 'NP'),('.', '.')], \n",
    "                        [('the', 'AT'), ('Republican', 'NP'), ('politicians', 'NNS'), ('.', '.')],\n",
    "                        [('the', 'AT'), ('American', 'NP'), ('people', 'NNS'), ('.', '.')]]\n",
    "\n",
    "\n",
    "    brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance'])\n",
    "    \n",
    "    all_tagged_sents = modified_speech_sents + brown_tagged_sents\n",
    "    return train_tagger(all_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909 pos accuracy on test set\n"
     ]
    }
   ],
   "source": [
    "brown_tagger = train_tagger_on_brown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tagger for the input text using the backoff tagger **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Trained_Speech_Tagger(sents,tagger):\n",
    "    if(isinstance(sents[0],list)):\n",
    "        return [tagger.tag(sent) for sent in sents]\n",
    "    else:\n",
    "        return tagger.tag(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funtion to retrieve the Bigram Collocations from the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_collocation_creator(text):\n",
    "    finder = BigramCollocationFinder.from_words(text)\n",
    "    return finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funtion to retrieve the Trigram Collocations from the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram_collocation_creator(text):\n",
    "    finder = TrigramCollocationFinder.from_words(text)\n",
    "    return finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Code to find the most common unigrams in the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_unigrams(sents):\n",
    "   lemmatizer = WordNetLemmatizer() # to get word stems\n",
    "   \n",
    "   \n",
    "   \n",
    "   normalized_words = [lemmatizer.lemmatize(word[0].lower()) for sent in sents\n",
    "                          for word in sent \n",
    "                          if word[1].startswith('N')]\n",
    "\n",
    "   top_unigrams = [word for (word, count) in nltk.FreqDist(normalized_words).most_common(40)]\n",
    "   return top_unigrams\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Categories Extraction through Hypernyms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categories_from_hypernyms(sents):\n",
    "    termlist = common_unigrams(sents) \n",
    "    hypterms = []\n",
    "    hypterms_dict = defaultdict(type([1]))\n",
    "    for term in termlist:                  \n",
    "        string = wn.synsets(term.lower(), 'n')  \n",
    "        for syn in string:                      \n",
    "            for hyp in syn.hypernyms():\n",
    "                hypterms = hypterms + [hyp.name]     \n",
    "                hypterms_dict[hyp.name].append(term)  \n",
    "    frequency = nltk.FreqDist(hypterms)\n",
    "    return frequency, hypterms_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to Extract Relevant information from personal text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in my text file - Speeches.txt which contains all of the Donald Trump speeches for his 2016 presdential campagin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = readtextfile(\"speeches.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Preprocessing to clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_text = text_preprocessing_trumpspeeches(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_sentences = sentence_tokenizer(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the text into relevant tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_tokens = word_tokenizer(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagging of the sentences and the tokens using the standard tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "standard_tagged_sentences = standard_pos_tagger(text_sentences)\n",
    "standard_tagged_tokens = standard_pos_tagger(text_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagging of the sentences and the tokens using the trained backoff tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "isinstance() arg 2 must be a type or tuple of types",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-8480b42041cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_tagged_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrained_Speech_Tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_sentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbrown_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrained_tagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrained_Speech_Tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbrown_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-161-d0306b08f103>\u001b[0m in \u001b[0;36mTrained_Speech_Tagger\u001b[0;34m(sents, tagger)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mTrained_Speech_Tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: isinstance() arg 2 must be a type or tuple of types"
     ]
    }
   ],
   "source": [
    "trained_tagged_sentences = Trained_Speech_Tagger(text_sentences,brown_tagger)\n",
    "trained_tagged_tokens = Trained_Speech_Tagger(text_tokens,brown_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up the tagged sentences to remove the stop words and punctuations. This function was initially performed before tagging, however, it made more sense to tag the text using the backoff tagger based on the complete text since stop words and and punctuations proved useful for correct tagging of certain elements. As a result, once tagged we go ahead and remove stop words, punctuations, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modified_tagged_sentences = [normalize_text(sent) for sent in trained_tagged_sentences]\n",
    "modified_tagged_tokens = normalize_text(trained_tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up the list of untagged tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenlist = [item for item in text_tokens if item not in punctuations and item.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the Bigram Collocation from the text which occur atleast 10 times in the text.  We remove the bigrams which include 'VBG' verb types from the text and find the remaining most common bigram collocations and store them in relevant bigram_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigram_collocation = bigram_collocation_creator(modified_tagged_tokens)\n",
    "bigram_collocation.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relevant_bigram_chunks = []\n",
    "for _list in bigram_collocation.ngram_fd.most_common(30):\n",
    "    if (_list[0][0][1] == \"VBG\" or _list[0][1][1] == \"VBG\"):\n",
    "        continue\n",
    "    else:\n",
    "        relevant_bigram_chunks.append(_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_list1 = []\n",
    "temp_list2 = []\n",
    "for item in relevant_bigram_chunks:\n",
    "    for _list in item:\n",
    "        temp_list1.append(_list[0])\n",
    "    new_phrase = ' '.join(temp_list1)\n",
    "    temp_list1 = []\n",
    "    temp_list2.append(new_phrase)\n",
    "\n",
    "relevant_bigram_chunks = temp_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Bigrams and arranging them based on PMI and then CHI and only retaining top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_collocation.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relevant_bigram_chunks_pmi = []\n",
    "temp_list1 = []\n",
    "temp_list2 = []\n",
    "for _list in bigram_collocation.nbest(bigram_measures.pmi, 10):\n",
    "    for item in _list:\n",
    "        temp_list1.append(item[0])\n",
    "    new_phrase = ' '.join(temp_list1)\n",
    "    temp_list1 = []\n",
    "    temp_list2.append(new_phrase)\n",
    "\n",
    "relevant_bigram_chunks_pmi = temp_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relevant_bigram_chunks_chi = []\n",
    "temp_list1 = []\n",
    "temp_list2 = []\n",
    "for _list in bigram_collocation.nbest(bigram_measures.chi_sq, 10):\n",
    "    for item in _list:\n",
    "        temp_list1.append(item[0])\n",
    "    new_phrase = ' '.join(temp_list1)\n",
    "    temp_list1 = []\n",
    "    temp_list2.append(new_phrase)\n",
    "\n",
    "relevant_bigram_chunks_chi = temp_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing common elements from all three results sets to get uniquely identifying Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resulting_list_bigrams = relevant_bigram_chunks_pmi\n",
    "resulting_list_bigrams.extend(x for x in relevant_bigram_chunks_chi if x not in resulting_list_bigrams)\n",
    "resulting_list_bigrams.extend(x for x in relevant_bigram_chunks if x not in resulting_list_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the Trigram Collocation from the text which occur atleast 10 times in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram_collocation = trigram_collocation_creator(modified_tagged_tokens)\n",
    "trigram_collocation.apply_freq_filter(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relevant_trigram_chunks = []\n",
    "temp_list1 = []\n",
    "temp_list = []\n",
    "temp_list2 = []\n",
    "for _list in trigram_collocation.ngram_fd.most_common(20):\n",
    "    temp_list1.append(_list[0])\n",
    "\n",
    "for item in temp_list1:\n",
    "    for x in item:\n",
    "        temp_list.append(x[0])\n",
    "    new_phrase = ' '.join(temp_list)\n",
    "    temp_list = []\n",
    "    temp_list2.append(new_phrase)\n",
    "\n",
    "relevant_trigram_chunks = temp_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relevant_trigram_chunks_pmi = []\n",
    "temp_list1 = []\n",
    "temp_list2 = []\n",
    "for _list in trigram_collocation.nbest(trigram_measures.pmi, 10):\n",
    "    for item in _list:\n",
    "        temp_list1.append(item[0])\n",
    "    new_phrase = ' '.join(temp_list1)\n",
    "    temp_list1 = []\n",
    "    temp_list2.append(new_phrase)\n",
    "\n",
    "relevant_trigram_chunks_pmi = temp_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing common elements from all three results sets to get uniquely identifying Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resulting_list_trigrams = relevant_trigram_chunks\n",
    "resulting_list_trigrams.extend(x for x in relevant_trigram_chunks_pmi if x not in resulting_list_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding trigrams within the original token list and looking at the most frequent trigrams to compare with above result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_list1 = []\n",
    "temp_list2 = []\n",
    "tr = nltk.trigrams(tokenlist)\n",
    "tr_fd = nltk.FreqDist(tr)\n",
    "tr_fd.most_common(20)\n",
    "for item in tr_fd.most_common(20):\n",
    "    for _list in item[0]:\n",
    "        temp_list1.append(_list)\n",
    "    new_phrase = ' '.join(temp_list1)\n",
    "    temp_list1 = []\n",
    "    temp_list2.append(new_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Relevant Chunks from the text based on a defined Noun Phrase Grammar Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mychunks = my_chunker(trained_tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nounphrase_list = []\n",
    "for tree in mychunks:\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'NounPhrase': \n",
    "            nounphrase_list.append(subtree.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunked using noun phrases (including prepositions) and retaining the most common ones but only those with at least 4 words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updated_nounphrase_list = []\n",
    "temp_list = []\n",
    "for item in nounphrase_list:\n",
    "    if (len(item) >= 4):\n",
    "        for list in item:\n",
    "            temp_list.append(list[0])\n",
    "        new_phrase = ' '.join(temp_list)\n",
    "        temp_list = []\n",
    "        updated_nounphrase_list.append(new_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(updated_nounphrase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relevant_chunks = [item[0] for item in fdist.most_common(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now return the most common hyperterms of the most frequent unigrams along with their synset examples to find some relevant sub topics and text attributes based on these synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperterm_frequency, hypterms_dict = categories_from_hypernyms(modified_tagged_sentences)\n",
    "common_terms = dict()\n",
    "for (name, count) in hyperterm_frequency.most_common(10):\n",
    "    nm=name().split('.')[0]\n",
    "    common_terms[nm]=', '.join(set(hypterms_dict[name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the Gist of the Personal Text based on above algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key categories generated from text to help give context to determined key phrases and key chunks\n",
      "\n",
      "Category: difficulty --> Example terms: problem, wall, job\n",
      "Category: transaction --> Example terms: deal, trade\n",
      "Category: administrative_district --> Example terms: state, country\n",
      "Category: work --> Example terms: care, job\n",
      "Category: artifact --> Example terms: thing, way\n",
      "Category: people --> Example terms: country, world, folk\n",
      "Category: group --> Example terms: people, world\n",
      "Category: time_period --> Example terms: day, time, year\n",
      "Category: large_indefinite_quantity --> Example terms: lot, deal, million\n",
      "Category: attribute --> Example terms: state, thing, time\n",
      "\n",
      "                 Key Phrases\n",
      "0            going take care\n",
      "1         make America great\n",
      "2           going build wall\n",
      "3         make country great\n",
      "4           Thank Thank much\n",
      "5           going bring back\n",
      "6              going get rid\n",
      "7         going happen going\n",
      "8       Thank much everybody\n",
      "9       going happen anymore\n",
      "10        going make country\n",
      "11        going make America\n",
      "12            take care vets\n",
      "13           going win going\n",
      "14             win going win\n",
      "15       happen going happen\n",
      "16         make country rich\n",
      "17           bring jobs back\n",
      "18          owe $19 trillion\n",
      "19             20 000 people\n",
      "20       Des Moines Register\n",
      "21          Jerry Falwell Jr\n",
      "22  George Washington Bridge\n",
      "23       Wall Street Journal\n",
      "24         toughest gun laws\n",
      "25        World Trade Center\n",
      "26            Take oh tender\n",
      "27                June 16 th\n",
      "28           September 11 th\n",
      "29           oh tender woman\n",
      "30            going pay wall\n",
      "31          Mexico going pay\n",
      "32          Thank much Thank\n",
      "                                    Key Chunks\n",
      "0                      WE'RE NOT CLOSING GITMO\n",
      "1                   Hillary Clinton s campaign\n",
      "2                         IT'S A RIGGED SYSTEM\n",
      "3                              SO HERE IS WHAT\n",
      "4                               AND BY THE WAY\n",
      "5                            THE WALL IS VITAL\n",
      "6                    America s great potential\n",
      "7                               NOT A BIG DEAL\n",
      "8   BUT MY FATHER WOULD SAY TAKE THE LUMPS OUT\n",
      "9                         Old Post Office site\n",
      "10                       DIDN'T EVEN KNOW THAT\n",
      "11                           HERE IS THE STORY\n",
      "12                       YOU LEAVE THE MEETING\n",
      "13                            WE DON'T HAVE IT\n",
      "14                        SO HERE IS THE STORY\n",
      "15                   YOU'RE A LITTLE TOO ROUGH\n",
      "16       THEY ARE GOING TO BE VOTING FOR TRUMP\n",
      "17    WE'RE GOING TO SAVE OUR SECOND AMENDMENT\n",
      "18          WE'RE GOING TO BRING OUR JOBS BACK\n",
      "19                         \" Merry Christmas \"\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.expand_frame_repr', True)\n",
    "df1 = pd.DataFrame(resulting_list_trigrams, columns=['Key Phrases'])\n",
    "df2 = pd.DataFrame(relevant_chunks, columns=['Key Chunks'])\n",
    "print (\"Key categories generated from text to help give context to determined key phrases and key chunks\")\n",
    "print()\n",
    "for k, v in common_terms.items():\n",
    "    print(\"Category:\",k,\"-->\",\"Example terms:\",v)\n",
    "print()\n",
    "print(df1)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflections\n",
    "\n",
    "It was easy to see that for the text that has been adopted - 'Donal Trump Speeches', a lot of information was transactional in nature. Heavy usage of verbs indicatin Trump's promise to perform a certain action affected the algorithm output and hence, that needed to be taken into consideration early on. \n",
    "\n",
    "Also given the poltiical nature of the text, there was heavy reliance on Proper Nouns in the text to bring across certain points regarding the presenditial campaign and this again was important to keep in mind when extracting the Gist of the text through chunking and related activities.\n",
    "\n",
    "There were very interesting insights gathered through this exercise and such as Trump's heavy reliance on the action word - \"going\" which again as mentioned above indicates promise.\n",
    "\n",
    "For the above algorithm, as mentioned throughout the notebook, there were certain points where the results were not as good as expected and hence newer techniques needed to be adopted. \n",
    "\n",
    "Extraction of frequent Bigrams and Unigrams was less indicative of any imporant information for my personal text and hence since frequent Bigrams included a lot of Proper Nouns without much context to them. As a result it proved helpful to find Frequent trigrams which gave god results in terms of descriptive key phrases. Also in the case of chunking, it was important to note that this proved to be the most helpful in performing key phrase extraction. Given the limitation of size, I limited my result set, since I wanted to combine it with other results. Frequent hypernym extraction proved to be a bit less useful again due to the nature of the text and the way Trump talks. However, we were still able to extract some key pieces of information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run below code for mystery text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = 'link'\n",
    "\n",
    "## Example -- file = 'http://people.ischool.berkeley.edu/~tygar/for.i206/pg1342.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file = read_file_url(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = readtextfile(\"mystery_text_expository_2016.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_mystery_text = text_preprocessing_trumpspeeches(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_sentences = sentence_tokenizer(processed_mystery_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_tokens = word_tokenizer(processed_mystery_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trained_tagged_sentences = Trained_Speech_Tagger(text_sentences,brown_tagger)\n",
    "trained_tagged_tokens = Trained_Speech_Tagger(text_tokens,brown_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modified_tagged_sentences = [normalize_text(sent) for sent in trained_tagged_sentences]\n",
    "modified_tagged_tokens = normalize_text(trained_tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_collocation = bigram_collocation_creator(modified_tagged_tokens)\n",
    "bigram_collocation.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_bigram_chunks = []\n",
    "for _list in bigram_collocation.ngram_fd.most_common(30):\n",
    "    if (_list[0][0][1] == \"VBG\" or _list[0][1][1] == \"VBG\"):\n",
    "        continue\n",
    "    else:\n",
    "        relevant_bigram_chunks.append(_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_list1 = []\n",
    "temp_list2 = []\n",
    "for item in relevant_bigram_chunks:\n",
    "    for _list in item:\n",
    "        temp_list1.append(_list[0])\n",
    "    new_phrase = ' '.join(temp_list1)\n",
    "    temp_list1 = []\n",
    "    temp_list2.append(new_phrase)\n",
    "\n",
    "relevant_bigram_chunks = temp_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_collocation.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_bigram_chunks_pmi = []\n",
    "temp_list1 = []\n",
    "temp_list2 = []\n",
    "for _list in bigram_collocation.nbest(bigram_measures.pmi, 10):\n",
    "    for item in _list:\n",
    "        temp_list1.append(item[0])\n",
    "    new_phrase = ' '.join(temp_list1)\n",
    "    temp_list1 = []\n",
    "    temp_list2.append(new_phrase)\n",
    "\n",
    "relevant_bigram_chunks_pmi = temp_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_bigram_chunks_chi = []\n",
    "temp_list1 = []\n",
    "temp_list2 = []\n",
    "for _list in bigram_collocation.nbest(bigram_measures.chi_sq, 10):\n",
    "    for item in _list:\n",
    "        temp_list1.append(item[0])\n",
    "    new_phrase = ' '.join(temp_list1)\n",
    "    temp_list1 = []\n",
    "    temp_list2.append(new_phrase)\n",
    "\n",
    "relevant_bigram_chunks_chi = temp_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resulting_list_bigrams = relevant_bigram_chunks_pmi\n",
    "resulting_list_bigrams.extend(x for x in relevant_bigram_chunks_chi if x not in resulting_list_bigrams)\n",
    "resulting_list_bigrams.extend(x for x in relevant_bigram_chunks if x not in resulting_list_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_collocation = trigram_collocation_creator(modified_tagged_tokens)\n",
    "trigram_collocation.apply_freq_filter(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_trigram_chunks = []\n",
    "temp_list1 = []\n",
    "temp_list = []\n",
    "temp_list2 = []\n",
    "for _list in trigram_collocation.ngram_fd.most_common(20):\n",
    "    temp_list1.append(_list[0])\n",
    "\n",
    "for item in temp_list1:\n",
    "    for x in item:\n",
    "        temp_list.append(x[0])\n",
    "    new_phrase = ' '.join(temp_list)\n",
    "    temp_list = []\n",
    "    temp_list2.append(new_phrase)\n",
    "\n",
    "relevant_trigram_chunks = temp_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_trigram_chunks_pmi = []\n",
    "temp_list1 = []\n",
    "temp_list2 = []\n",
    "for _list in trigram_collocation.nbest(trigram_measures.pmi, 10):\n",
    "    for item in _list:\n",
    "        temp_list1.append(item[0])\n",
    "    new_phrase = ' '.join(temp_list1)\n",
    "    temp_list1 = []\n",
    "    temp_list2.append(new_phrase)\n",
    "\n",
    "relevant_trigram_chunks_pmi = temp_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resulting_list_trigrams = relevant_trigram_chunks\n",
    "resulting_list_trigrams.extend(x for x in relevant_trigram_chunks_pmi if x not in resulting_list_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mychunks = my_chunker(trained_tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nounphrase_list = []\n",
    "for tree in mychunks:\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'NounPhrase': \n",
    "            nounphrase_list.append(subtree.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "updated_nounphrase_list = []\n",
    "temp_list = []\n",
    "for item in nounphrase_list:\n",
    "    if (len(item) >= 4):\n",
    "        for list in item:\n",
    "            temp_list.append(list[0])\n",
    "        new_phrase = ' '.join(temp_list)\n",
    "        temp_list = []\n",
    "        updated_nounphrase_list.append(new_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(updated_nounphrase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_chunks = [item[0] for item in fdist.most_common(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperterm_frequency, hypterms_dict = categories_from_hypernyms(modified_tagged_sentences)\n",
    "common_terms = dict()\n",
    "for (name, count) in hyperterm_frequency.most_common(10):\n",
    "    nm=name().split('.')[0]\n",
    "    common_terms[nm]=', '.join(set(hypterms_dict[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key categories generated from text to help give context to determined key phrases and key chunks\n",
      "\n",
      "Category: administrative_district --> Example terms: state, department, country\n",
      "Category: people --> Example terms: world, nation, country, business\n",
      "Category: activity --> Example terms: effort, work, service, business\n",
      "Category: group --> Example terms: world, system, people, men\n",
      "Category: state --> Example terms: action, power, condition\n",
      "Category: time_period --> Example terms: time, year, life\n",
      "Category: political_unit --> Example terms: state, nation, country\n",
      "Category: being --> Example terms: life\n",
      "Category: force --> Example terms: law, service, men\n",
      "Category: male --> Example terms: men\n",
      "\n",
      "                       Key Phrases\n",
      "0              State Union Address\n",
      "1                 fiscal year 1947\n",
      "2         Government United States\n",
      "3             people United States\n",
      "4                    21 st century\n",
      "5                   ending June 30\n",
      "6                     World War II\n",
      "7            United States America\n",
      "8   Interstate Commerce Commission\n",
      "9                 year ending June\n",
      "10                 past four years\n",
      "11         State local governments\n",
      "12             current fiscal year\n",
      "13    Senate House Representatives\n",
      "14             dollars fiscal year\n",
      "15                next fiscal year\n",
      "16        United States Government\n",
      "17             last annual message\n",
      "18                   Mr Speaker Mr\n",
      "19          Congress United States\n",
      "20     Literary Archive Foundation\n",
      "21               Child Left Behind\n",
      "22      Gutenberg Literary Archive\n",
      "23           Five Civilized Tribes\n",
      "24                   P. O'Neill Jr\n",
      "25               Lyndon B. Johnson\n",
      "26               Thomas P. O'Neill\n",
      "27          Equal Rights Amendment\n",
      "28      Project Gutenberg Literary\n",
      "29              William J. Clinton\n",
      "                                     Key Chunks\n",
      "0                                   * * * State\n",
      "1   Union Address Franklin D. Roosevelt January\n",
      "2         Union Address Woodrow Wilson December\n",
      "3         Union Address Harry S. Truman January\n",
      "4     Union Address Theodore Roosevelt December\n",
      "5    Union Address Dwight D. Eisenhower January\n",
      "6      Union Address William J. Clinton January\n",
      "7                     GENTLEMEN OF THE CONGRESS\n",
      "8        Union Address Calvin Coolidge December\n",
      "9       Union Address Lyndon B. Johnson January\n",
      "10          Union Address Ronald Reagan January\n",
      "11           International Atomic Energy Agency\n",
      "12        Union Address Herbert Hoover December\n",
      "13          Union Address Richard Nixon January\n",
      "14              States Civil Service Commission\n",
      "15       Union Address William H. Taft December\n",
      "16             National Industrial Recovery Act\n",
      "17      Union Address William McKinley December\n",
      "18              Reciprocal Trade Agreements Act\n",
      "19         Union Address George W. Bush January\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.expand_frame_repr', True)\n",
    "df1 = pd.DataFrame(resulting_list_trigrams, columns=['Key Phrases'])\n",
    "df2 = pd.DataFrame(relevant_chunks, columns=['Key Chunks'])\n",
    "print (\"Key categories generated from text to help give context to determined key phrases and key chunks\")\n",
    "print()\n",
    "for k, v in common_terms.items():\n",
    "    print(\"Category:\",k,\"-->\",\"Example terms:\",v)\n",
    "print()\n",
    "print(df1)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
